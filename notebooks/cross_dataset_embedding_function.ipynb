{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-dataset evaluation of universal embedding function for traffic classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install faiss with GPU support\n",
    "Installing *faiss* with GPU support can be a bit complicated. See the official instructions - https://github.com/facebookresearch/faiss/blob/main/INSTALL.md. To run this notebook, we recommend starting with a fresh conda env by running the following:\n",
    "\n",
    "```bash\n",
    "conda create -n cross_dataset_faiss_env python=3.10 ipykernel ipywidgets\n",
    "conda install -c pytorch -c nvidia pytorch pytorch-cuda=12.4\n",
    "conda install -c conda-forge faiss-gpu=1.8.0 numpy=1.26.4\n",
    "```\n",
    "\n",
    "This installs faiss and PyTorch with conda and the rest is installed with pip. For Linux, there are unofficial *faiss* wheels available here - https://pypi.org/project/faiss-gpu-cu12/#description, but we did not tested them.\n",
    "\n",
    "If installing the GPU version is not possible, use:\n",
    "```bash\n",
    "pip install faiss-cpu\n",
    "```\n",
    "\n",
    "#### Install common dependencies for both Windows and Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cesnet_models\n",
      "  Using cached cesnet_models-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting cesnet_datazoo\n",
      "  Using cached cesnet_datazoo-0.1.10-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torchinfo\n",
      "  Using cached torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy<2.0 in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from cesnet_models) (1.26.4)\n",
      "Collecting scikit-learn (from cesnet_models)\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.10 in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from cesnet_models) (2.5.1)\n",
      "Collecting matplotlib (from cesnet_datazoo)\n",
      "  Using cached matplotlib-3.10.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pandas (from cesnet_datazoo)\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pydantic<=2.8.2,>=2.0 (from cesnet_datazoo)\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from cesnet_datazoo) (6.0.2)\n",
      "Collecting requests (from cesnet_datazoo)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting seaborn (from cesnet_datazoo)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting tables<=3.9.2,>=3.8.0 (from cesnet_datazoo)\n",
      "  Using cached tables-3.9.2-cp310-cp310-win_amd64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<=2.8.2,>=2.0->cesnet_datazoo)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<=2.8.2,>=2.0->cesnet_datazoo)\n",
      "  Using cached pydantic_core-2.20.1-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from pydantic<=2.8.2,>=2.0->cesnet_datazoo) (4.12.2)\n",
      "Collecting numexpr>=2.6.2 (from tables<=3.9.2,>=3.8.0->cesnet_datazoo)\n",
      "  Using cached numexpr-2.10.2-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from tables<=3.9.2,>=3.8.0->cesnet_datazoo) (24.2)\n",
      "Collecting py-cpuinfo (from tables<=3.9.2,>=3.8.0->cesnet_datazoo)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting blosc2>=2.3.0 (from tables<=3.9.2,>=3.8.0->cesnet_datazoo)\n",
      "  Using cached blosc2-2.7.1-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from torch>=1.10->cesnet_models) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from torch>=1.10->cesnet_models) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from torch>=1.10->cesnet_models) (3.1.4)\n",
      "Collecting fsspec (from torch>=1.10->cesnet_models)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.10->cesnet_models)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from sympy==1.13.1->torch>=1.10->cesnet_models) (1.3.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->cesnet_datazoo)\n",
      "  Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->cesnet_datazoo)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->cesnet_datazoo)\n",
      "  Using cached fonttools-4.55.3-cp310-cp310-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->cesnet_datazoo)\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib->cesnet_datazoo)\n",
      "  Using cached pillow-11.1.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->cesnet_datazoo)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from matplotlib->cesnet_datazoo) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->cesnet_datazoo)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->cesnet_datazoo)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->cesnet_datazoo)\n",
      "  Using cached charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->cesnet_datazoo)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->cesnet_datazoo)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->cesnet_datazoo)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->cesnet_models)\n",
      "  Using cached scipy-1.15.1-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->cesnet_models)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->cesnet_models)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting ndindex>=1.4 (from blosc2>=2.3.0->tables<=3.9.2,>=3.8.0->cesnet_datazoo)\n",
      "  Using cached ndindex-1.9.2-cp310-cp310-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting msgpack (from blosc2>=2.3.0->tables<=3.9.2,>=3.8.0->cesnet_datazoo)\n",
      "  Using cached msgpack-1.1.0-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->cesnet_datazoo) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\janlu\\miniconda3\\envs\\cross_dataset_faiss_env\\lib\\site-packages (from jinja2->torch>=1.10->cesnet_models) (2.1.3)\n",
      "Using cached cesnet_models-0.4.0-py3-none-any.whl (38 kB)\n",
      "Using cached cesnet_datazoo-0.1.10-py3-none-any.whl (51 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pydantic_core-2.20.1-cp310-none-win_amd64.whl (1.9 MB)\n",
      "Using cached tables-3.9.2-cp310-cp310-win_amd64.whl (4.4 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached matplotlib-3.10.0-cp310-cp310-win_amd64.whl (8.0 MB)\n",
      "Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached blosc2-2.7.1-cp310-cp310-win_amd64.whl (2.4 MB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Using cached contourpy-1.3.1-cp310-cp310-win_amd64.whl (218 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.55.3-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Using cached numexpr-2.10.2-cp310-cp310-win_amd64.whl (144 kB)\n",
      "Using cached pillow-11.1.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached scipy-1.15.1-cp310-cp310-win_amd64.whl (43.9 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached ndindex-1.9.2-cp310-cp310-win_amd64.whl (159 kB)\n",
      "Using cached msgpack-1.1.0-cp310-cp310-win_amd64.whl (74 kB)\n",
      "Installing collected packages: pytz, py-cpuinfo, urllib3, tzdata, tqdm, torchinfo, threadpoolctl, sympy, scipy, pyparsing, pydantic-core, pillow, numexpr, ndindex, msgpack, kiwisolver, joblib, idna, fsspec, fonttools, cycler, contourpy, charset-normalizer, certifi, annotated-types, scikit-learn, requests, pydantic, pandas, matplotlib, blosc2, tables, seaborn, cesnet_models, cesnet_datazoo\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed annotated-types-0.7.0 blosc2-2.7.1 certifi-2024.12.14 cesnet_datazoo-0.1.10 cesnet_models-0.4.0 charset-normalizer-3.4.1 contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 fsspec-2024.12.0 idna-3.10 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.0 msgpack-1.1.0 ndindex-1.9.2 numexpr-2.10.2 pandas-2.2.3 pillow-11.1.0 py-cpuinfo-9.0.0 pydantic-2.8.2 pydantic-core-2.20.1 pyparsing-3.2.1 pytz-2024.2 requests-2.32.3 scikit-learn-1.6.1 scipy-1.15.1 seaborn-0.13.2 sympy-1.13.1 tables-3.9.2 threadpoolctl-3.5.0 torchinfo-1.8.0 tqdm-4.67.1 tzdata-2024.2 urllib3-2.3.0\n",
      "Faiss with GPU support is available\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install cesnet_models cesnet_datazoo tqdm torchinfo\n",
    "\n",
    "# import faiss\n",
    "# if hasattr(faiss, \"StandardGpuResources\"):\n",
    "#     print(\"Faiss with GPU support is available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tcbench install on Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install tcbench "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tcbench install on Windows\n",
    "The *tcbench* framework depends on Aim for experiment tracking. Aim is not supported on Windows - https://github.com/aimhubio/aim/issues/2064.\n",
    "The workaround is to install *tcbench* dependencies without Aim, which works because the experiment tracking functionality is not needed for downloading datasets and obtaining the provided train, validation, and test splits.\n",
    "\n",
    "After installing *tcbench* like this, you need to comment out all imports of Aim. For tcbench==0.0.22, Aim imports need to be commented out in the following files:\n",
    "\n",
    "* cli/command_aimrepo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tcbench\n",
      "  Using cached tcbench-0.0.22-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting rich\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting rich_click\n",
      "  Using cached rich_click-1.8.5-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting click_plugins\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pyarrow==12.0.0\n",
      "  Using cached pyarrow-12.0.0-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Using cached pyarrow-12.0.0-cp310-cp310-win_amd64.whl (21.5 MB)\n",
      "Using cached tcbench-0.0.22-py3-none-any.whl (115 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached rich_click-1.8.5-py3-none-any.whl (35 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Installing collected packages: click_plugins, tcbench, rich_click, rich, pyarrow, click\n",
      "Successfully installed click-8.1.8 click_plugins-1.1.1 pyarrow-12.0.0 rich-13.9.4 rich_click-1.8.5 tcbench-0.0.22\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --no-deps tcbench rich rich_click click click_plugins pyarrow==12.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download datasets\n",
    "Install tcbench datasets in command line within the activated conda env:\n",
    "```bash\n",
    "tcbench datasets import --name ucdavis-icdm19\n",
    "tcbench datasets import --name utmobilenet21\n",
    "tcbench datasets install --name mirage19\n",
    "tcbench datasets install --name mirage22\n",
    "```\n",
    "The installation of the MIRAGE22 dataset can sometimes fail while generating the splits because it runs out of RAM (for a 32GB machine). A possible solution is installing this dataset on a remote machine with more RAM and downloading the parquet files.\n",
    "\n",
    "Check that all datasets are installed (data splits filled) with:\n",
    "```bash\n",
    "tcbench datasets info\n",
    "```\n",
    "\n",
    "And proceed with installing the CESNET-TLS22 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CESNET-TLS22-S dataset\n",
      "File size: 3.01GB\n",
      "Remaining: 3.01GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.01G/3.01G [05:56<00:00, 9.07MB/s]\n"
     ]
    }
   ],
   "source": [
    "# from cesnet_datazoo.datasets import CESNET_TLS22\n",
    "# dataset = CESNET_TLS22(data_root=\"data/CESNET-TLS22/\", size=\"S\", silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tcbench as tcb\n",
    "from tcbench.libtcdatasets.utmobilenet21_generate_splits import _verify_splits\n",
    "\n",
    "TCBENCH_APP_COLUMN = \"app\"\n",
    "PPI_MAX_LEN = 30\n",
    "PPI_IPT_POS = 0\n",
    "PPI_DIR_POS = 1\n",
    "PPI_SIZE_POS = 2\n",
    "\n",
    "\n",
    "def tcbench_convert_ppi(row, is_utmobilenet: bool = False):\n",
    "    directions = np.where(row[\"pkts_dir\"] == 0, -1, 1)\n",
    "    sizes = row[\"pkts_size\"]\n",
    "    if is_utmobilenet:\n",
    "        # For UTMOBILENET21, the time differences are already in the \"timetofirst\" column\n",
    "        time_differences = row[\"timetofirst\"].copy()\n",
    "    else:\n",
    "        time_differences = np.diff(row[\"timetofirst\"], prepend=0)\n",
    "        assert len(directions) == len(sizes) == len(time_differences)\n",
    "        assert np.isclose(time_differences.cumsum(), row[\"timetofirst\"]).all()\n",
    "        if \"pkts_iat\" in row:\n",
    "            assert np.isclose(time_differences, row[\"pkts_iat\"]).all()\n",
    "        time_differences[0] = 0.0\n",
    "    if \"duration\" in row:\n",
    "        assert np.isclose(row[\"duration\"], time_differences.sum())\n",
    "    time_differences = time_differences * 1000 # convert to ms\n",
    "    # cesnet-models expects the following PPI format: (IPT, DIR, SIZE)\n",
    "    ppi = (time_differences, directions, sizes)\n",
    "    ppi = np.array(ppi)[:, :PPI_MAX_LEN]\n",
    "    ppi = np.pad(ppi, pad_width=((0, 0), (0, PPI_MAX_LEN - len(ppi[0]))))\n",
    "    return ppi\n",
    "\n",
    "def get_data_from_tcbench(dataset_enum: tcb.DATASETS, split_id: int = 0, ucdavis_test_set: Optional[str] = None, use_val_as_train: bool = False) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if dataset_enum == tcb.DATASETS.UCDAVISICDM19:\n",
    "        if ucdavis_test_set is None:\n",
    "            raise ValueError(\"ucdavis_test_set must be either 'script' or 'human' when using the UCDAVIS19 dataset\")\n",
    "        # Use following to start using the tcbench prepared train splits for UCDAVIS19\n",
    "        # df_train = tcb.load_parquet(dataset_enum, split=split_id) # type: ignore\n",
    "        df = tcb.load_parquet(dataset_enum)\n",
    "        df_train = df[df[\"partition\"] == \"pretraining\"]\n",
    "        df_test = tcb.load_parquet(dataset_enum, split=ucdavis_test_set)\n",
    "    else:\n",
    "        df = tcb.load_parquet(dataset_enum, min_pkts=10)\n",
    "        df_splits = tcb.load_parquet(dataset_enum, min_pkts=10, split=True) # type: ignore\n",
    "        _verify_splits(df, df_splits)\n",
    "        split_indices = df_splits.iloc[split_id]\n",
    "        train_incides, val_indices, test_indices = split_indices[\"train_indexes\"], split_indices[\"val_indexes\"], split_indices[\"test_indexes\"]\n",
    "        df_train, df_val, df_test = df.iloc[train_incides], df.iloc[val_indices], df.iloc[test_indices]\n",
    "        if use_val_as_train:\n",
    "            df_train = pd.concat([df_train, df_val])\n",
    "    ppi_fn = partial(tcbench_convert_ppi, is_utmobilenet=dataset_enum==tcb.DATASETS.UTMOBILENET21)\n",
    "    train_data, test_data = np.stack(df_train.apply(ppi_fn, axis=1)), np.stack(df_test.apply(ppi_fn, axis=1))\n",
    "    train_labels, test_labels = df_train[TCBENCH_APP_COLUMN].to_numpy(), df_test[TCBENCH_APP_COLUMN].to_numpy()\n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cesnet_datazoo.config import DatasetConfig\n",
    "from cesnet_datazoo.constants import APP_COLUMN, PPI_COLUMN\n",
    "from cesnet_datazoo.datasets import CESNET_TLS22\n",
    "\n",
    "\n",
    "def load_cesnet_tls22_from_datazoo(dataset_size: str = \"S\", split_id: int = 0, train_size: int = 1_000_000, test_size: int = 1_000_000) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    assert test_size < 10_000_000\n",
    "    dataset = CESNET_TLS22(data_root=\"data/CESNET-TLS22/\", size=dataset_size, silent=True)\n",
    "    dataset_config = DatasetConfig(\n",
    "        dataset=dataset,\n",
    "        fold_id=split_id,\n",
    "        batch_size=16384,\n",
    "        test_batch_size=16384,\n",
    "        train_period_name=\"W-2021-40\",\n",
    "        test_period_name=\"W-2021-41\",\n",
    "        train_size=train_size,\n",
    "        test_known_size=\"all\",\n",
    "        train_workers=0,\n",
    "        test_workers=0,\n",
    "        need_val_set=False,)\n",
    "    dataset.set_dataset_config_and_initialize(dataset_config)\n",
    "    assert dataset.class_info is not None\n",
    "    df_train = dataset.get_train_df()\n",
    "    df_test = dataset.get_test_df().sample(test_size, random_state=42 + split_id)\n",
    "    train_data, test_data = np.stack(df_train[PPI_COLUMN]), np.stack(df_test[PPI_COLUMN])\n",
    "    train_labels, test_labels = dataset.class_info.encoder.inverse_transform(df_train[APP_COLUMN]), dataset.class_info.encoder.inverse_transform(df_test[APP_COLUMN])\n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class DatasetWithTransform(Dataset):\n",
    "    ppi_transform: Callable\n",
    "    labels: np.ndarray\n",
    "    data: torch.Tensor\n",
    "\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, ppi_transform: Callable) -> None:\n",
    "        assert len(data) == len(labels)\n",
    "        self.ppi_transform = ppi_transform\n",
    "        self.labels = labels\n",
    "        self.data = torch.from_numpy(self.ppi_transform(data).astype(\"float32\"))\n",
    "\n",
    "    def __getitem__(self, index) -> torch.Tensor:\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_embeddings_from_loaded_dataset(model: nn.Module, dataloader: DataLoader, device, silent: bool = False) -> tuple[np.ndarray, np.ndarray]:\n",
    "    assert isinstance(dataloader.dataset, DatasetWithTransform)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch_ppi in tqdm(dataloader, total=len(dataloader), disable=silent):\n",
    "            batch_ppi = batch_ppi.to(device)\n",
    "            batch_embeddings = model(batch_ppi)\n",
    "            embeddings.append(batch_embeddings)\n",
    "    embeddings = torch.cat(embeddings).cpu().numpy()\n",
    "    return embeddings, dataloader.dataset.labels\n",
    "\n",
    "def prepare_dataloader(data, labels, ppi_transform, batch_size=2048):\n",
    "    dataset = DatasetWithTransform(data=data, labels=labels, ppi_transform=ppi_transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "def find_ranks_faiss(vecs, qvecs, device: torch.device, metric: str = \"cosine\", N: int = 5, batch_size: Optional[int] = None, silent: bool = False) -> tuple[np.ndarray, np.ndarray]:\n",
    "    if metric == \"cosine\":\n",
    "        index = faiss.IndexFlatIP(vecs.shape[-1])\n",
    "    elif metric == \"L1\":\n",
    "        index = faiss.IndexFlat(vecs.shape[-1], faiss.METRIC_L1)\n",
    "    if device.type == \"cuda\" and hasattr(faiss, \"StandardGpuResources\"):\n",
    "        torch.cuda.empty_cache()\n",
    "        gpu_res = faiss.StandardGpuResources()\n",
    "        index = faiss.index_cpu_to_gpu(gpu_res, 0, index)\n",
    "\n",
    "    index.add(vecs) # type: ignore\n",
    "    if batch_size is None:\n",
    "        scores, ranks = index.search(qvecs, N) # type: ignore\n",
    "    else:\n",
    "        num_batches = (len(qvecs) // batch_size) + 1\n",
    "        scores_list = []\n",
    "        ranks_list = []\n",
    "        for batch in tqdm(np.array_split(qvecs, num_batches), total=num_batches, disable=silent):\n",
    "            scores, ranks = index.search(batch, N) # type: ignore\n",
    "            scores_list.append(scores)\n",
    "            ranks_list.append(ranks)\n",
    "        scores = np.concatenate(scores_list, axis=0)\n",
    "        ranks = np.concatenate(ranks_list, axis=0)\n",
    "    return scores, ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_space_embeddings(data, num_packets: int = 30, ipt_max_clip: int = 1000, ipt_scale: float = 1.0, dir_scale: float = 1.0):\n",
    "    data = data[:, :, :num_packets]\n",
    "    sizes = data[:, PPI_SIZE_POS].clip(min=0, max=1500)\n",
    "    dirs = data[:, PPI_DIR_POS] * dir_scale\n",
    "    times = data[:, PPI_IPT_POS].clip(min=0, max=ipt_max_clip) * ipt_scale\n",
    "    embeddings =  np.hstack((dirs, sizes, times))\n",
    "    return embeddings\n",
    "\n",
    "def replace_unseen_packet_embeddings(embedding_model, replace_threshold: int = 1, small_packets_replace_with: int = 0, silent: bool = False) -> None:\n",
    "    backbone_model = embedding_model.backbone_model\n",
    "    if not hasattr(backbone_model, \"psizes_hist\"):\n",
    "        print(\"Histogram of training packet sizes is not available\")\n",
    "        return\n",
    "    df_train_packets = pd.DataFrame(backbone_model.psizes_hist, columns=[\"Count\"])\n",
    "    df_train_packets[\"Perc\"] = df_train_packets[\"Count\"] / df_train_packets[\"Count\"].sum()\n",
    "    packets_to_replace = df_train_packets[df_train_packets[\"Count\"] < replace_threshold].index\n",
    "    if len(packets_to_replace) == 0:\n",
    "        print(f\"All packet sizes were seen at least {replace_threshold} times\")\n",
    "        return\n",
    "    # Small <100 unseen packets are replaced with the embedding of 'small_packets_replace_with'\n",
    "    for i in packets_to_replace[packets_to_replace < 100]: # type: ignore\n",
    "        backbone_model.packet_size_nn_embedding.weight.data[i] = backbone_model.packet_size_nn_embedding.weight.data[small_packets_replace_with]\n",
    "        if not silent: print(f\"Setting the packet size embedding of {i} ({df_train_packets.Count.iloc[i]} obs) to {small_packets_replace_with} ({df_train_packets.Count.iloc[small_packets_replace_with]} obs)\")\n",
    "    # Big >=1250 unseen packets are replaced with their closest seen packet\n",
    "    seen_big_packets = [i for i in range(1250, 1501) if i not in packets_to_replace]\n",
    "    for i in packets_to_replace[packets_to_replace >= 1250]: # type: ignore\n",
    "        replace_with = min(seen_big_packets, key=lambda x: abs(x - i)) # type: ignore\n",
    "        if not silent: print(f\"Setting the packet size embedding of {i} ({df_train_packets.Count.iloc[i]} obs) to {replace_with} ({df_train_packets.Count.iloc[replace_with]} obs)\")\n",
    "        backbone_model.packet_size_nn_embedding.weight.data[i] = backbone_model.packet_size_nn_embedding.weight.data[replace_with]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "EmbeddingModel                                [2048, 256]               --\n",
       "├─Multimodal_CESNET_Enhanced: 1-1             --                        --\n",
       "│    └─Embedding: 2-1                         [2048, 30, 20]            30,020\n",
       "│    └─Embedding: 2-2                         [2048, 30, 10]            2,000\n",
       "│    └─Identity: 2-3                          [2048, 32, 30]            --\n",
       "│    └─Sequential: 2-4                        [2048, 448, 30]           --\n",
       "│    │    └─Bottleneck: 3-1                   [2048, 192, 30]           --\n",
       "│    │    │    └─Sequential: 4-1              [2048, 192, 30]           --\n",
       "│    │    │    │    └─Identity: 5-1           [2048, 32, 30]            --\n",
       "│    │    │    │    └─PadConv1d: 5-2          [2048, 192, 30]           6,144\n",
       "│    │    │    │    └─BatchNorm1d: 5-3        [2048, 192, 30]           384\n",
       "│    │    │    └─PadConv1d: 4-2               [2048, 48, 30]            1,536\n",
       "│    │    │    └─BatchNorm1d: 4-3             [2048, 48, 30]            96\n",
       "│    │    │    └─ReLU: 4-4                    [2048, 48, 30]            --\n",
       "│    │    │    └─PadConv1d: 4-5               [2048, 48, 30]            16,128\n",
       "│    │    │    └─BatchNorm1d: 4-6             [2048, 48, 30]            96\n",
       "│    │    │    └─ReLU: 4-7                    [2048, 48, 30]            --\n",
       "│    │    │    └─PadConv1d: 4-8               [2048, 192, 30]           9,216\n",
       "│    │    │    └─BatchNorm1d: 4-9             [2048, 192, 30]           384\n",
       "│    │    │    └─Identity: 4-10               [2048, 192, 30]           --\n",
       "│    │    │    └─ReLU: 4-11                   [2048, 192, 30]           --\n",
       "│    │    └─Bottleneck: 3-2                   [2048, 256, 30]           --\n",
       "│    │    │    └─Sequential: 4-12             [2048, 256, 30]           --\n",
       "│    │    │    │    └─Identity: 5-4           [2048, 192, 30]           --\n",
       "│    │    │    │    └─PadConv1d: 5-5          [2048, 256, 30]           49,152\n",
       "│    │    │    │    └─BatchNorm1d: 5-6        [2048, 256, 30]           512\n",
       "│    │    │    └─PadConv1d: 4-13              [2048, 64, 30]            12,288\n",
       "│    │    │    └─BatchNorm1d: 4-14            [2048, 64, 30]            128\n",
       "│    │    │    └─ReLU: 4-15                   [2048, 64, 30]            --\n",
       "│    │    │    └─PadConv1d: 4-16              [2048, 64, 30]            28,672\n",
       "│    │    │    └─BatchNorm1d: 4-17            [2048, 64, 30]            128\n",
       "│    │    │    └─ReLU: 4-18                   [2048, 64, 30]            --\n",
       "│    │    │    └─PadConv1d: 4-19              [2048, 256, 30]           16,384\n",
       "│    │    │    └─BatchNorm1d: 4-20            [2048, 256, 30]           512\n",
       "│    │    │    └─Dropout: 4-21                [2048, 256, 30]           --\n",
       "│    │    │    └─ReLU: 4-22                   [2048, 256, 30]           --\n",
       "│    │    └─Bottleneck: 3-3                   [2048, 384, 30]           --\n",
       "│    │    │    └─Sequential: 4-23             [2048, 384, 30]           --\n",
       "│    │    │    │    └─Identity: 5-7           [2048, 256, 30]           --\n",
       "│    │    │    │    └─PadConv1d: 5-8          [2048, 384, 30]           98,304\n",
       "│    │    │    │    └─BatchNorm1d: 5-9        [2048, 384, 30]           768\n",
       "│    │    │    └─PadConv1d: 4-24              [2048, 96, 30]            24,576\n",
       "│    │    │    └─BatchNorm1d: 4-25            [2048, 96, 30]            192\n",
       "│    │    │    └─ReLU: 4-26                   [2048, 96, 30]            --\n",
       "│    │    │    └─PadConv1d: 4-27              [2048, 96, 30]            46,080\n",
       "│    │    │    └─BatchNorm1d: 4-28            [2048, 96, 30]            192\n",
       "│    │    │    └─ReLU: 4-29                   [2048, 96, 30]            --\n",
       "│    │    │    └─PadConv1d: 4-30              [2048, 384, 30]           36,864\n",
       "│    │    │    └─BatchNorm1d: 4-31            [2048, 384, 30]           768\n",
       "│    │    │    └─Dropout: 4-32                [2048, 384, 30]           --\n",
       "│    │    │    └─ReLU: 4-33                   [2048, 384, 30]           --\n",
       "│    │    └─Bottleneck: 3-4                   [2048, 448, 30]           --\n",
       "│    │    │    └─Sequential: 4-34             [2048, 448, 30]           --\n",
       "│    │    │    │    └─Identity: 5-10          [2048, 384, 30]           --\n",
       "│    │    │    │    └─PadConv1d: 5-11         [2048, 448, 30]           172,032\n",
       "│    │    │    │    └─BatchNorm1d: 5-12       [2048, 448, 30]           896\n",
       "│    │    │    └─PadConv1d: 4-35              [2048, 112, 30]           43,008\n",
       "│    │    │    └─BatchNorm1d: 4-36            [2048, 112, 30]           224\n",
       "│    │    │    └─ReLU: 4-37                   [2048, 112, 30]           --\n",
       "│    │    │    └─PadConv1d: 4-38              [2048, 112, 30]           37,632\n",
       "│    │    │    └─BatchNorm1d: 4-39            [2048, 112, 30]           224\n",
       "│    │    │    └─ReLU: 4-40                   [2048, 112, 30]           --\n",
       "│    │    │    └─PadConv1d: 4-41              [2048, 448, 30]           50,176\n",
       "│    │    │    └─BatchNorm1d: 4-42            [2048, 448, 30]           896\n",
       "│    │    │    └─Dropout: 4-43                [2048, 448, 30]           --\n",
       "│    │    │    └─ReLU: 4-44                   [2048, 448, 30]           --\n",
       "│    └─Sequential: 2-5                        [2048, 448]               --\n",
       "│    │    └─AdaptiveGeM: 3-5                  [2048, 448, 1]            1\n",
       "│    │    └─Flatten: 3-6                      [2048, 448]               --\n",
       "│    │    └─Identity: 3-7                     [2048, 448]               --\n",
       "│    │    └─Identity: 3-8                     [2048, 448]               --\n",
       "│    └─Sequential: 2-6                        [2048, 448]               --\n",
       "│    │    └─Linear: 3-9                       [2048, 448]               201,152\n",
       "│    │    └─BatchNorm1d: 3-10                 [2048, 448]               896\n",
       "│    │    └─Identity: 3-11                    [2048, 448]               --\n",
       "│    │    └─ReLU: 3-12                        [2048, 448]               --\n",
       "├─Linear: 1-2                                 [2048, 256]               114,944\n",
       "├─BatchNorm1d: 1-3                            [2048, 256]               512\n",
       "===============================================================================================\n",
       "Total params: 1,004,117\n",
       "Trainable params: 1,004,117\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 40.55\n",
       "===============================================================================================\n",
       "Input size (MB): 0.74\n",
       "Forward/backward pass size (MB): 3190.88\n",
       "Params size (MB): 4.02\n",
       "Estimated Total Size (MB): 3195.64\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cesnet_models.models import (Model_30pktTCNET_256_Weights,\n",
    "                                  model_30pktTCNET_256)\n",
    "from torchinfo import summary\n",
    "\n",
    "SMALL_PACKETS_REPLACE_WITH = 0\n",
    "REPLACE_THRESHOLD = 1\n",
    "\n",
    "\n",
    "pretrained_weights = Model_30pktTCNET_256_Weights.DEFAULT\n",
    "ppi_transform = pretrained_weights.transforms[\"ppi_transform\"]\n",
    "embedding_model = model_30pktTCNET_256(weights=pretrained_weights)\n",
    "replace_unseen_packet_embeddings(embedding_model, replace_threshold=REPLACE_THRESHOLD, small_packets_replace_with=SMALL_PACKETS_REPLACE_WITH, silent=True)\n",
    "summary(embedding_model.to(\"cuda\"), input_size=(2048, 3, 30), depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset UCDAVIS19-script with splits [0, 1, 2, 3, 4] (printing output for the first split)\n",
      "Creating embeddings for train and test sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 12.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 181.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ranking with faiss\n",
      "Time elapsed for faiss ranking: 0.12 s\n",
      "Embedding model top1-acc 100.00, maj-acc 99.33\n",
      "Computing input space ranking with faiss\n",
      "Time elapsed for faiss ranking: 0.12 s\n",
      "Input space baseline top1-acc 98.00, maj-acc 97.33\n",
      "\n",
      "Processing dataset UCDAVIS19-human with splits [0, 1, 2, 3, 4] (printing output for the first split)\n",
      "Creating embeddings for train and test sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 10.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 226.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ranking with faiss\n",
      "Time elapsed for faiss ranking: 0.11 s\n",
      "Embedding model top1-acc 81.93, maj-acc 81.93\n",
      "Computing input space ranking with faiss\n",
      "Time elapsed for faiss ranking: 0.11 s\n",
      "Input space baseline top1-acc 71.08, maj-acc 71.08\n",
      "\n",
      "Processing dataset UTMOBILENET21 with splits [0, 1, 2, 3, 4] (printing output for the first split)\n",
      "Creating embeddings for train and test sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 35.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 105.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ranking with faiss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for faiss ranking: 0.11 s\n",
      "Embedding model top1-acc 87.00, maj-acc 87.95\n",
      "Computing input space ranking with faiss\n",
      "Time elapsed for faiss ranking: 0.12 s\n",
      "Input space baseline top1-acc 84.25, maj-acc 84.99\n",
      "\n",
      "Processing dataset MIRAGE19 with splits [0, 1, 2, 3, 4] (printing output for the first split)\n",
      "Creating embeddings for train and test sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:01<00:00, 22.29it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 36.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ranking with faiss\n",
      "Time elapsed for faiss ranking: 0.15 s\n",
      "Embedding model top1-acc 84.04, maj-acc 83.69\n",
      "Computing input space ranking with faiss\n",
      "Time elapsed for faiss ranking: 0.18 s\n",
      "Input space baseline top1-acc 80.32, maj-acc 79.92\n",
      "\n",
      "Processing dataset MIRAGE22 with splits [0, 1, 2, 3, 4] (printing output for the first split)\n",
      "Creating embeddings for train and test sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 19.28it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 34.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ranking with faiss\n",
      "Time elapsed for faiss ranking: 0.13 s\n",
      "Embedding model top1-acc 97.87, maj-acc 97.39\n",
      "Computing input space ranking with faiss\n",
      "Time elapsed for faiss ranking: 0.12 s\n",
      "Input space baseline top1-acc 95.48, maj-acc 94.96\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "EVALUATE_EMBEDDING_MODEL = True\n",
    "RANKING_N = 5\n",
    "SPLITS = [0, 1, 2, 3, 4]\n",
    "MAJVOTE_K = 3\n",
    "\n",
    "EVALUATE_INPUT_SPACE_BASELINE = True\n",
    "BASELINE_PACKETS = 10\n",
    "BASELINE_IPT_MAX_CLIP = 1000 # ms\n",
    "BASELINE_IPT_SCALE = 0.1\n",
    "BASELINE_DIR_SCALE = 1\n",
    "\n",
    "datasets = {\n",
    "    \"UCDAVIS19-script\": tcb.DATASETS.UCDAVISICDM19,\n",
    "    \"UCDAVIS19-human\": tcb.DATASETS.UCDAVISICDM19,\n",
    "    \"UTMOBILENET21\": tcb.DATASETS.UTMOBILENET21,\n",
    "    \"MIRAGE19\": tcb.DATASETS.MIRAGE19,\n",
    "    \"MIRAGE22\": tcb.DATASETS.MIRAGE22,\n",
    "    # \"CESNET-TLS22\": \"CESNET-TLS22\",\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_model = embedding_model.to(device)\n",
    "per_dataset_model_metrics: dict = defaultdict(lambda: {\"top1-acc\": [], \"top1-recall\": [], \"maj-acc\": [], \"maj-recall\": []})\n",
    "per_dataset_baseline_metrics: dict = defaultdict(lambda: {\"top1-acc\": [], \"top1-recall\": [], \"maj-acc\": [], \"maj-recall\": []})\n",
    "for dataset_name, dataset_enum in datasets.items():\n",
    "    print(f\"\\nProcessing dataset {dataset_name} with splits {SPLITS} (printing output for the first split)\")\n",
    "    for split_id in SPLITS:\n",
    "        if dataset_name == \"CESNET-TLS22\":\n",
    "            train_data, test_data, train_labels, test_labels = load_cesnet_tls22_from_datazoo(split_id=split_id)\n",
    "        else:\n",
    "            # UCDAVIS19 currently uses the entire pretraining partition as the training set\n",
    "            if dataset_name == \"UCDAVIS19-human\":\n",
    "                ucdavis_test_set = \"human\"\n",
    "            elif dataset_name == \"UCDAVIS19-script\":\n",
    "                ucdavis_test_set = \"script\"\n",
    "            else:\n",
    "                ucdavis_test_set = None\n",
    "            train_data, test_data, train_labels, test_labels = get_data_from_tcbench(dataset_enum, split_id=split_id, ucdavis_test_set=ucdavis_test_set)\n",
    "        train_dataloader = prepare_dataloader(data=train_data, labels=train_labels, ppi_transform=ppi_transform)\n",
    "        test_dataloader = prepare_dataloader(data=test_data, labels=test_labels, ppi_transform=ppi_transform)\n",
    "        if split_id == 0: print(\"Creating embeddings for train and test sets\")\n",
    "        train_embeddings, _ = compute_embeddings_from_loaded_dataset(embedding_model, dataloader=train_dataloader, device=device, silent=split_id != 0)\n",
    "        test_embeddings, _ = compute_embeddings_from_loaded_dataset(embedding_model, dataloader=test_dataloader, device=device, silent=split_id != 0)\n",
    "\n",
    "        if EVALUATE_EMBEDDING_MODEL:\n",
    "            start_time = time.time()\n",
    "            if split_id == 0: print(\"Computing ranking with faiss\")\n",
    "            distances, ranks = find_ranks_faiss(vecs=train_embeddings,\n",
    "                                                qvecs=test_embeddings,\n",
    "                                                N=RANKING_N,\n",
    "                                                batch_size=10_000 if dataset_name == \"CESNET-TLS22\" else None,\n",
    "                                                silent=split_id != 0,\n",
    "                                                device=device,)\n",
    "            if split_id == 0: print(f\"Time elapsed for faiss ranking: {time.time() - start_time:.2f} s\")\n",
    "            # Compute metrics based on the ranking\n",
    "            closest_1 = train_labels[ranks[:, 0]]\n",
    "            maj_vote = [collections.Counter(row).most_common(1)[0][0] for row in train_labels[ranks[:, :MAJVOTE_K]]]\n",
    "            top1_acc = (test_labels == closest_1).mean()\n",
    "            maj_acc = (test_labels == maj_vote).mean()\n",
    "            top1_recall = recall_score(test_labels, closest_1, average=\"macro\", zero_division=0)\n",
    "            maj_recall = recall_score(test_labels, maj_vote, average=\"macro\", zero_division=0)\n",
    "            per_dataset_model_metrics[dataset_name][\"top1-acc\"].append(top1_acc)\n",
    "            per_dataset_model_metrics[dataset_name][\"top1-recall\"].append(top1_recall)\n",
    "            per_dataset_model_metrics[dataset_name][\"maj-recall\"].append(maj_recall)\n",
    "            per_dataset_model_metrics[dataset_name][\"maj-acc\"].append(maj_acc)\n",
    "            if split_id == 0: print(f\"Embedding model top1-acc {top1_acc * 100:.2f}, maj-acc {maj_acc * 100:.2f}\")\n",
    "\n",
    "        if EVALUATE_INPUT_SPACE_BASELINE:\n",
    "            start_time = time.time()\n",
    "            baseline_train_embeddings = prepare_input_space_embeddings(train_data,\n",
    "                                                                       num_packets=BASELINE_PACKETS,\n",
    "                                                                       ipt_max_clip=BASELINE_IPT_MAX_CLIP,\n",
    "                                                                       ipt_scale=BASELINE_IPT_SCALE,\n",
    "                                                                       dir_scale=BASELINE_DIR_SCALE,)\n",
    "            baseline_test_embeddings = prepare_input_space_embeddings(test_data,\n",
    "                                                                      num_packets=BASELINE_PACKETS,\n",
    "                                                                      ipt_max_clip=BASELINE_IPT_MAX_CLIP,\n",
    "                                                                      ipt_scale=BASELINE_IPT_SCALE,\n",
    "                                                                      dir_scale=BASELINE_DIR_SCALE,)\n",
    "            if split_id == 0: print(\"Computing input space ranking with faiss\")\n",
    "            baseline_distances, baseline_ranks = find_ranks_faiss(baseline_train_embeddings,\n",
    "                                                                  baseline_test_embeddings,\n",
    "                                                                  N=RANKING_N,\n",
    "                                                                  metric=\"L1\",\n",
    "                                                                  batch_size=10_000 if dataset_name == \"CESNET-TLS22\" else None,\n",
    "                                                                  silent=split_id != 0,\n",
    "                                                                  device=device,)\n",
    "            if split_id == 0: print(f\"Time elapsed for faiss ranking: {time.time() - start_time:.2f} s\")\n",
    "            # Compute metrics based on the ranking\n",
    "            baseline_closest_1 = train_labels[baseline_ranks[:, 0]]\n",
    "            baseline_maj_vote = [collections.Counter(row).most_common(1)[0][0] for row in train_labels[baseline_ranks[:, :MAJVOTE_K]]]\n",
    "            baseline_top1_acc = (test_labels == baseline_closest_1).mean()\n",
    "            baseline_maj_acc = (test_labels == baseline_maj_vote).mean()\n",
    "            baseline_top1_recall = recall_score(test_labels, baseline_closest_1, average=\"macro\", zero_division=0)\n",
    "            baseline_maj_recall = recall_score(test_labels, baseline_maj_vote, average=\"macro\", zero_division=0)\n",
    "            per_dataset_baseline_metrics[dataset_name][\"top1-acc\"].append(baseline_top1_acc)\n",
    "            per_dataset_baseline_metrics[dataset_name][\"top1-recall\"].append(baseline_top1_recall)\n",
    "            per_dataset_baseline_metrics[dataset_name][\"maj-acc\"].append(baseline_maj_acc)\n",
    "            per_dataset_baseline_metrics[dataset_name][\"maj-recall\"].append(baseline_maj_recall)\n",
    "            if split_id == 0: print(f\"Input space baseline top1-acc {baseline_top1_acc * 100:.2f}, maj-acc {baseline_maj_acc * 100:.2f}\")\n",
    "    # Average metrics across splits\n",
    "    per_dataset_model_metrics[dataset_name] = {metric: np.mean(per_split_values) for metric, per_split_values in per_dataset_model_metrics[dataset_name].items()}\n",
    "    per_dataset_baseline_metrics[dataset_name] = {metric: np.mean(per_split_values) for metric, per_split_values in per_dataset_baseline_metrics[dataset_name].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Space</th>\n",
       "      <th>Input Space Delta</th>\n",
       "      <th>Top-1 Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UCDAVIS19-script</th>\n",
       "      <td>98.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCDAVIS19-human</th>\n",
       "      <td>71.08</td>\n",
       "      <td>10.85</td>\n",
       "      <td>81.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTMOBILENET21</th>\n",
       "      <td>83.55</td>\n",
       "      <td>3.13</td>\n",
       "      <td>86.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIRAGE19</th>\n",
       "      <td>80.01</td>\n",
       "      <td>3.72</td>\n",
       "      <td>83.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIRAGE22</th>\n",
       "      <td>95.63</td>\n",
       "      <td>2.14</td>\n",
       "      <td>97.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Input Space  Input Space Delta  Top-1 Accuracy\n",
       "UCDAVIS19-script        98.00               2.00          100.00\n",
       "UCDAVIS19-human         71.08              10.85           81.93\n",
       "UTMOBILENET21           83.55               3.13           86.68\n",
       "MIRAGE19                80.01               3.72           83.73\n",
       "MIRAGE22                95.63               2.14           97.77"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Space</th>\n",
       "      <th>Input Space Delta</th>\n",
       "      <th>Maj-3 Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UCDAVIS19-script</th>\n",
       "      <td>97.33</td>\n",
       "      <td>2.00</td>\n",
       "      <td>99.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCDAVIS19-human</th>\n",
       "      <td>71.08</td>\n",
       "      <td>10.85</td>\n",
       "      <td>81.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTMOBILENET21</th>\n",
       "      <td>84.04</td>\n",
       "      <td>2.94</td>\n",
       "      <td>86.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIRAGE19</th>\n",
       "      <td>79.20</td>\n",
       "      <td>3.95</td>\n",
       "      <td>83.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIRAGE22</th>\n",
       "      <td>95.21</td>\n",
       "      <td>2.24</td>\n",
       "      <td>97.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Input Space  Input Space Delta  Maj-3 Accuracy\n",
       "UCDAVIS19-script        97.33               2.00           99.33\n",
       "UCDAVIS19-human         71.08              10.85           81.93\n",
       "UTMOBILENET21           84.04               2.94           86.98\n",
       "MIRAGE19                79.20               3.95           83.15\n",
       "MIRAGE22                95.21               2.24           97.45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Space</th>\n",
       "      <th>Input Space Delta</th>\n",
       "      <th>Top-1 Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UCDAVIS19-script</th>\n",
       "      <td>98.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCDAVIS19-human</th>\n",
       "      <td>70.00</td>\n",
       "      <td>11.11</td>\n",
       "      <td>81.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTMOBILENET21</th>\n",
       "      <td>72.75</td>\n",
       "      <td>3.62</td>\n",
       "      <td>76.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIRAGE19</th>\n",
       "      <td>75.80</td>\n",
       "      <td>4.32</td>\n",
       "      <td>80.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIRAGE22</th>\n",
       "      <td>95.70</td>\n",
       "      <td>2.11</td>\n",
       "      <td>97.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Input Space  Input Space Delta  Top-1 Recall\n",
       "UCDAVIS19-script        98.00               2.00        100.00\n",
       "UCDAVIS19-human         70.00              11.11         81.11\n",
       "UTMOBILENET21           72.75               3.62         76.37\n",
       "MIRAGE19                75.80               4.32         80.12\n",
       "MIRAGE22                95.70               2.11         97.81"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Space</th>\n",
       "      <th>Input Space Delta</th>\n",
       "      <th>Maj-3 Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UCDAVIS19-script</th>\n",
       "      <td>97.33</td>\n",
       "      <td>2.00</td>\n",
       "      <td>99.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCDAVIS19-human</th>\n",
       "      <td>70.33</td>\n",
       "      <td>10.56</td>\n",
       "      <td>80.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UTMOBILENET21</th>\n",
       "      <td>72.90</td>\n",
       "      <td>3.43</td>\n",
       "      <td>76.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIRAGE19</th>\n",
       "      <td>74.96</td>\n",
       "      <td>4.37</td>\n",
       "      <td>79.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIRAGE22</th>\n",
       "      <td>95.28</td>\n",
       "      <td>2.22</td>\n",
       "      <td>97.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Input Space  Input Space Delta  Maj-3 Recall\n",
       "UCDAVIS19-script        97.33               2.00         99.33\n",
       "UCDAVIS19-human         70.33              10.56         80.89\n",
       "UTMOBILENET21           72.90               3.43         76.33\n",
       "MIRAGE19                74.96               4.37         79.33\n",
       "MIRAGE22                95.28               2.22         97.50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def prepare_metrics_df(per_dataset_model_metrics, per_dataset_baseline_metrics, metric: str, column_name: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame.from_dict({d: m[metric] for d, m in per_dataset_model_metrics.items()}, orient=\"index\", columns=[column_name])\n",
    "    df[column_name] = (df[column_name] * 100).round(2)\n",
    "    df.insert(0, \"Input Space\", df.index.map(lambda d: round(per_dataset_baseline_metrics[d][metric] * 100, 2)))\n",
    "    df.insert(1, \"Input Space Delta\", df[column_name] - df[\"Input Space\"])\n",
    "    return df\n",
    "\n",
    "df_top1_acc = prepare_metrics_df(per_dataset_model_metrics, per_dataset_baseline_metrics, \"top1-acc\", \"Top-1 Accuracy\")\n",
    "df_top1_recall = prepare_metrics_df(per_dataset_model_metrics, per_dataset_baseline_metrics, \"top1-recall\", \"Top-1 Recall\")\n",
    "df_maj_acc = prepare_metrics_df(per_dataset_model_metrics, per_dataset_baseline_metrics, \"maj-acc\", f\"Maj-{MAJVOTE_K} Accuracy\")\n",
    "df_maj_recall = prepare_metrics_df(per_dataset_model_metrics, per_dataset_baseline_metrics, \"maj-recall\", f\"Maj-{MAJVOTE_K} Recall\")\n",
    "\n",
    "display(df_top1_acc)\n",
    "display(df_maj_acc)\n",
    "display(df_top1_recall)\n",
    "display(df_maj_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross_dataset_faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
